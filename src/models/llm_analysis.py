from datetime import datetime
from typing import Optional
from abc import ABC, abstractmethod
from langchain_openai import ChatOpenAI
from langchain_google_vertexai import ChatVertexAI
from langchain.prompts import PromptTemplate
from langchain_core.prompts import ChatPromptTemplate
import json

# ì„¤ì • ê°€ì ¸ì˜¤ê¸°
from src.config.config import (
    OPENAI_API_KEY,
    LLM_MODEL,
    LLM_TEMPERATURE,
    LLM_MAX_TOKENS,
    GOOGLE_CLOUD_PROJECT,
    GOOGLE_CLOUD_LOCATION,
    VERTEX_AI_MODEL,
    VERTEX_AI_TEMPERATURE,
    VERTEX_AI_MAX_TOKENS
)
from src.prompts.meeting_analysis_prompts import SYSTEM_PROMPT, USER_PROMPT
from src.prompts.planning_meeting_prompts import SYSTEM_PROMPT as PLANNING_SYSTEM_PROMPT, USER_PROMPT as PLANNING_USER_PROMPT
from src.utils.schema import MeetingAnalysis, PlanningMeetingAnalysis


class BaseMeetingAnalyzer(ABC):    
    def __init__(self):
        pass
    
    @abstractmethod
    def _get_model_name(self) -> str:
        """ëª¨ë¸ëª… ë°˜í™˜ (ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        pass
    
    def analyze_comprehensive(self, transcript: str, questions: list = None, speaker_stats: dict = None) -> str:

        try:
            # ì§ˆë¬¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            questions_text = ""
            if questions:
                if isinstance(questions, list):
                    questions_text = "\n".join(f"- {q}" for q in questions)
                else:
                    questions_text = str(questions)
            
            # í™”ì í†µê³„ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            speaker_stats_text = ""
            if speaker_stats:
                speaker_stats_text = "\ní™”ìë³„ ë°œì–¸ ì ìœ ìœ¨:\n"
                for speaker_name, stats in speaker_stats.items():
                    speaker_stats_text += f"- {speaker_name}: {stats['percentage']}% ({stats['formatted_time']})\n"
            
            # ì „ì‚¬ í…ìŠ¤íŠ¸ì— í™”ì í†µê³„ ì¶”ê°€
            full_transcript = transcript
            if speaker_stats_text:
                full_transcript = f"{transcript}\n\n{speaker_stats_text}"
            
            # ê¸°ì¡´ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (JSON ì¶œë ¥ì€ ë¬´ì‹œë¨)
            user_prompt_template = PromptTemplate(
                input_variables=["transcript", "questions"],
                template=USER_PROMPT
            )
            
            # í”„ë¡¬í”„íŠ¸ ì²´ì¸ êµ¬ì„±
            prompt = ChatPromptTemplate.from_messages([
                ("system", SYSTEM_PROMPT),
                ("human", user_prompt_template.template)
            ])
            
            # ì²´ì¸ ìƒì„±: prompt | structured_llm
            chain = prompt | self.llm.with_structured_output(MeetingAnalysis)
            
            # ì‹¤í–‰
            print(f"ğŸ” Gemini ìš”ì²­ ë°ì´í„° í¬ê¸°: {len(full_transcript)}ì")
            print(f"ğŸ” ì§ˆë¬¸ ê°œìˆ˜: {len(questions) if questions else 0}ê°œ")
            
            result = chain.invoke({
                "transcript": full_transcript,
                "questions": questions_text
            })
            
            print(f"ğŸ” Gemini ì‘ë‹µ íƒ€ì…: {type(result)}")
            print(f"ğŸ” Gemini ì‘ë‹µ ë‚´ìš©: {str(result)[:200]}..." if result else "ğŸ” Gemini ì‘ë‹µ: None")
            
            # None ì²´í¬ ì¶”ê°€
            if result is None:
                print("âš ï¸ Geminiê°€ Noneì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨ ë˜ëŠ” ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
                raise ValueError("LLM returned None - possibly due to schema validation failure or content too long")
            
            # JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
            return result.model_dump_json(indent=2)
            
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            print(f"âŒ ë¶„ì„ ì˜¤ë¥˜ ìƒì„¸:\n{error_detail}")
            
            # ì—ëŸ¬ë„ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
            error_result = {
                "error": "ë¶„ì„ ì˜¤ë¥˜",
                "message": str(e),
                "detail": error_detail
            }
            return json.dumps(error_result, indent=2, ensure_ascii=False)
    
    def analyze_planning_meeting(self, transcript: str, questions: list = None, speaker_stats: dict = None) -> str:
        """ê¸°íšíšŒì˜ ë¶„ì„ ë©”ì„œë“œ"""
        try:
            # ì§ˆë¬¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            questions_text = ""
            if questions:
                if isinstance(questions, list):
                    questions_text = "\n".join(f"- {q}" for q in questions)
                else:
                    questions_text = str(questions)
            
            # í™”ì í†µê³„ í…ìŠ¤íŠ¸ ì²˜ë¦¬ (ê¸°íšíšŒì˜ì—ì„œëŠ” ì°¸ê³ ìš©)
            speaker_stats_text = ""
            if speaker_stats:
                speaker_stats_text = "\nì°¸ì—¬ìë³„ ë°œì–¸ ì ìœ ìœ¨:\n"
                for speaker_name, stats in speaker_stats.items():
                    speaker_stats_text += f"- {speaker_name}: {stats['percentage']}% ({stats['formatted_time']})\n"
            
            # ì „ì‚¬ í…ìŠ¤íŠ¸ì— í™”ì í†µê³„ ì¶”ê°€
            full_transcript = transcript
            if speaker_stats_text:
                full_transcript = f"{transcript}\n\n{speaker_stats_text}"
            
            # ê¸°íšíšŒì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©
            user_prompt_template = PromptTemplate(
                input_variables=["transcript", "questions"],
                template=PLANNING_USER_PROMPT
            )
            
            # í”„ë¡¬í”„íŠ¸ ì²´ì¸ êµ¬ì„±
            prompt = ChatPromptTemplate.from_messages([
                ("system", PLANNING_SYSTEM_PROMPT),
                ("human", user_prompt_template.template)
            ])
            
            # ì²´ì¸ ìƒì„±: prompt | structured_llm
            chain = prompt | self.llm.with_structured_output(PlanningMeetingAnalysis)
            
            # ì‹¤í–‰
            print(f"ğŸ” ê¸°íšíšŒì˜ ë¶„ì„ ìš”ì²­ ë°ì´í„° í¬ê¸°: {len(full_transcript)}ì")
            print(f"ğŸ” ì§ˆë¬¸ ê°œìˆ˜: {len(questions) if questions else 0}ê°œ")
            
            result = chain.invoke({
                "transcript": full_transcript,
                "questions": questions_text
            })
            
            print(f"ğŸ” ê¸°íšíšŒì˜ ë¶„ì„ ì‘ë‹µ íƒ€ì…: {type(result)}")
            print(f"ğŸ” ê¸°íšíšŒì˜ ë¶„ì„ ì‘ë‹µ ë‚´ìš©: {str(result)[:200]}..." if result else "ğŸ” ê¸°íšíšŒì˜ ë¶„ì„ ì‘ë‹µ: None")
            
            # None ì²´í¬ ì¶”ê°€
            if result is None:
                print("âš ï¸ ê¸°íšíšŒì˜ ë¶„ì„ì—ì„œ Noneì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
                raise ValueError("Planning meeting analysis returned None")
            
            # JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
            return result.model_dump_json(indent=2)
            
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            print(f"âŒ ê¸°íšíšŒì˜ ë¶„ì„ ì˜¤ë¥˜ ìƒì„¸:\n{error_detail}")
            
            # ì—ëŸ¬ë„ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
            error_result = {
                "error": "ê¸°íšíšŒì˜ ë¶„ì„ ì˜¤ë¥˜",
                "message": str(e),
                "detail": error_detail
            }
            return json.dumps(error_result, indent=2, ensure_ascii=False)
    


class OpenAIMeetingAnalyzer(BaseMeetingAnalyzer):    
    def __init__(self, api_key: Optional[str] = None):

        super().__init__()
        
        self.api_key = api_key or OPENAI_API_KEY
        if not self.api_key:
            raise ValueError("OpenAI API key is required")
        
        # OpenAI LLM ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            openai_api_key=self.api_key,
            model=LLM_MODEL,
            temperature=LLM_TEMPERATURE,
            max_tokens=LLM_MAX_TOKENS
        )
        print(f"âœ… OpenAI {LLM_MODEL} ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _get_model_name(self) -> str:
        """OpenAI ëª¨ë¸ëª… ë°˜í™˜"""
        return LLM_MODEL


class GeminiMeetingAnalyzer(BaseMeetingAnalyzer):    
    def __init__(self, google_project: Optional[str] = None, google_location: Optional[str] = None):

        super().__init__()
        
        self.google_project = google_project or GOOGLE_CLOUD_PROJECT
        self.google_location = google_location or GOOGLE_CLOUD_LOCATION
        
        if not self.google_project:
            raise ValueError("Google Cloud Project ID is required")
        
        # Vertex AI Gemini LLM ì´ˆê¸°í™”
        self.llm = ChatVertexAI(
            project=self.google_project,
            location=self.google_location,
            model_name=VERTEX_AI_MODEL,
            temperature=VERTEX_AI_TEMPERATURE,
            max_output_tokens=VERTEX_AI_MAX_TOKENS,
        )
        print(f"âœ… Vertex AI {VERTEX_AI_MODEL} ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _get_model_name(self) -> str:
        """Gemini ëª¨ë¸ëª… ë°˜í™˜"""
        return VERTEX_AI_MODEL
    
