"""Ïò§ÎîîÏò§ ÎÖπÏùå Î∞è Ï†ÑÏÇ¨ ÌÜµÌï© Î™®Îìà

Ïù¥ Î™®ÎìàÏùÄ Ïò§ÎîîÏò§ ÎÖπÏùåÍ≥º ÏùåÏÑ±-ÌÖçÏä§Ìä∏ Î≥ÄÌôò Ï≤òÎ¶¨Î•º ÏúÑÌïú Ìè¨Í¥ÑÏ†ÅÏù∏ ÏÜîÎ£®ÏÖòÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.

ÌÅ¥ÎûòÏä§Îì§:
    AudioRecorder: Ï†ÅÏ†àÌïú Î¶¨ÏÜåÏä§ Í¥ÄÎ¶¨ÏôÄ Ìï®Íªò Ïã§ÏãúÍ∞Ñ Ïò§ÎîîÏò§ ÎÖπÏùå Ï≤òÎ¶¨
    TranscriptionFormatter: ÌôîÏûê Î∂ÑÏÑùÍ≥º Ìï®Íªò Ï†ÑÏÇ¨ Í≤∞Í≥º Ìè¨Îß∑ÌåÖ
    STTProcessor: ÏóêÎü¨ Ï≤òÎ¶¨Í∞Ä Ìè¨Ìï®Îêú AssemblyAI Í∏∞Î∞ò ÏùåÏÑ±-ÌÖçÏä§Ìä∏ Ï≤òÎ¶¨
    AudioProcessor: ÎÖπÏùåÍ≥º Ï†ÑÏÇ¨Î•º ÌÜµÌï©Ìïú Í≥†ÏàòÏ§Ä Ïù∏ÌÑ∞ÌéòÏù¥Ïä§
    
ÏÉÅÏàòÎì§:
    ÎÖπÏùå ÏÑ§Ï†ï, ÌÉÄÏù¥Î∞ç, Í∑∏Î¶¨Í≥† Ìè¨Îß∑ÌåÖ ÏÉÅÏàòÎì§
"""

import assemblyai as aai
import os
import threading
import queue
import time
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple, Union
import numpy as np
import sounddevice as sd
import soundfile as sf
from dataclasses import dataclass
from enum import Enum

# ÏÑ§Ï†ï Í∞íÎì§ Í∞ÄÏ†∏Ïò§Í∏∞
from src.config.config import (
    # Ïò§ÎîîÏò§ Í¥ÄÎ†® ÏÑ§Ï†ï
    AUDIO_SAMPLE_RATE,
    AUDIO_CHANNELS,
    AUDIO_CHUNK_SIZE,
    AUDIO_FORMAT,
    TEMP_AUDIO_DIR,
    OUTPUT_DIR,
    # AssemblyAI ÏÑ§Ï†ï
    ASSEMBLYAI_API_KEY,
    ASSEMBLYAI_SPEECH_MODEL,
    ASSEMBLYAI_LANGUAGE,
    ASSEMBLYAI_PUNCTUATE,
    ASSEMBLYAI_FORMAT_TEXT,
    ASSEMBLYAI_DISFLUENCIES,
    ASSEMBLYAI_SPEAKER_LABELS,
    ASSEMBLYAI_LANGUAGE_DETECTION,
    ASSEMBLYAI_WORD_BOOST,
    ASSEMBLYAI_BOOST_PARAM,
    ASSEMBLYAI_SPEAKERS_EXPECTED
)

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ÏÉÅÏàòÎì§
class ProcessingStatus(Enum):
    """Ï≤òÎ¶¨ ÏÉÅÌÉú Í∞íÎì§ÏùÑ ÏúÑÌïú Ïó¥Í±∞Ìòï"""
    SUCCESS = "success"
    ERROR = "error"
    SUCCESS_NO_UTTERANCES = "success_but_no_utterances"
    PROCESSING = "processing"


# ÏÑ§Ï†ïÍ≥º ÏÉÅÏàòÎì§
RECORDING_TIMEOUT = 2.0  # ÎÖπÏùå Ïä§Î†àÎìú Ï¢ÖÎ£å ÎåÄÍ∏∞ ÏãúÍ∞Ñ(Ï¥à)
QUEUE_TIMEOUT = 0.1  # Ïò§ÎîîÏò§ ÌÅê ÎåÄÍ∏∞ ÏãúÍ∞Ñ(Ï¥à)
TRANSCRIPT_POLL_INTERVAL = 5  # Ï†ÑÏÇ¨ ÏÉÅÌÉú ÌôïÏù∏ Í∞ÑÍ≤©(Ï¥à)
FILE_TIMESTAMP_FORMAT = "%Y%m%d_%H%M%S"
ISO_TIMESTAMP_FORMAT = "%Y-%m-%dT%H:%M:%S"
TIME_DISPLAY_FORMAT = "{minutes}Î∂Ñ {seconds}Ï¥à"
SPEAKER_ALPHABET_START = 65  # ASCII 'A'


@dataclass
class SpeakerInfo:
    """ÌôîÏûê Ï†ïÎ≥¥Î•º ÏúÑÌïú Î∂àÎ≥Ä Îç∞Ïù¥ÌÑ∞ ÌÅ¥ÎûòÏä§"""
    id: str
    name: str
    role: str
    total_seconds: float
    formatted_time: str
    percentage: float


@dataclass 
class UtteranceInfo:
    """Î∞úÌôî Ï†ïÎ≥¥Î•º ÏúÑÌïú Î∂àÎ≥Ä Îç∞Ïù¥ÌÑ∞ ÌÅ¥ÎûòÏä§"""
    speaker: str
    start_time: float
    end_time: float
    duration: float
    text: str
    formatted_start: str


@dataclass
class TranscriptionResult:
    """Ï†ÑÏÇ¨ Í≤∞Í≥ºÎ•º ÏúÑÌïú Î∂àÎ≥Ä Îç∞Ïù¥ÌÑ∞ ÌÅ¥ÎûòÏä§"""
    transcript: str
    status: str
    timestamp: str
    speaker_times: Dict[str, Dict[str, Union[float, str]]]
    total_duration_seconds: float
    utterances: List[Dict[str, Any]]
    participants: List[Dict[str, str]]


class AudioRecorder:
    """Ï†ÅÏ†àÌïú Î¶¨ÏÜåÏä§ Í¥ÄÎ¶¨ÏôÄ ÏóêÎü¨ Ï≤òÎ¶¨Î•º Ìè¨Ìï®Ìïú Ïò§ÎîîÏò§ ÎÖπÏùå Ï≤òÎ¶¨"""
    
    def __init__(self) -> None:
        """ÏÑ§Ï†ïÍ∞íÏúºÎ°ú AudioRecorder Ï¥àÍ∏∞Ìôî"""
        self.sample_rate: int = AUDIO_SAMPLE_RATE
        self.channels: int = AUDIO_CHANNELS
        self.chunk_size: int = AUDIO_CHUNK_SIZE
        self.is_recording: bool = False
        self.audio_queue: queue.Queue = queue.Queue()
        self.recorded_frames: List[np.ndarray] = []
        self.stream: Optional[sd.InputStream] = None
        self.recording_thread: Optional[threading.Thread] = None
        
        # ÌïÑÏöîÌïú ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
        self._ensure_directories_exist()
        
        logger.info(f"AudioRecorder Ï¥àÍ∏∞Ìôî ÏôÑÎ£å: {self.sample_rate}Hz, {self.channels} Ï±ÑÎÑê")
    
    def _ensure_directories_exist(self) -> None:
        """ÌïÑÏöîÌïú ÎîîÎ†âÌÜ†Î¶¨Îì§Ïù¥ Ï°¥Ïû¨ÌïòÎäîÏßÄ ÌôïÏù∏"""
        try:
            Path(TEMP_AUDIO_DIR).mkdir(parents=True, exist_ok=True)
            Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)
            logger.debug(f"ÎîîÎ†âÌÜ†Î¶¨ ÌôïÏù∏ ÏôÑÎ£å: {TEMP_AUDIO_DIR}, {OUTPUT_DIR}")
        except Exception as e:
            logger.error(f"ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            raise
    
    def _audio_callback(self, indata: np.ndarray, frames: int, time_info: Any, status: sd.CallbackFlags) -> None:
        """ÏóêÎü¨ Ï≤òÎ¶¨Î•º Ìè¨Ìï®Ìïú Ïò§ÎîîÏò§ Ïä§Ìä∏Î¶º ÏΩúÎ∞± Ìï®Ïàò"""
        if status:
            logger.warning(f"Ïò§ÎîîÏò§ ÏΩúÎ∞± ÏÉÅÌÉú: {status}")
        
        if self.is_recording:
            try:
                self.audio_queue.put(indata.copy())
            except Exception as e:
                logger.error(f"Ïò§ÎîîÏò§ ÏΩúÎ∞± Ïò§Î•ò: {e}")
                self.is_recording = False
    
    def start_recording(self) -> bool:
        """Ï†ÅÏ†àÌïú ÏóêÎü¨ Ï≤òÎ¶¨Î•º Ìè¨Ìï®ÌïòÏó¨ Ïò§ÎîîÏò§ ÎÖπÏùå ÏãúÏûë"""
        if self.is_recording:
            logger.warning("Ïù¥ÎØ∏ ÎÖπÏùåÏù¥ ÏßÑÌñâ Ï§ëÏûÖÎãàÎã§")
            return False
            
        try:
            self._reset_recording_state()
            
            # Ïò§ÎîîÏò§ Ïä§Ìä∏Î¶º Ï¥àÍ∏∞Ìôî
            self.stream = sd.InputStream(
                callback=self._audio_callback,
                channels=self.channels,
                samplerate=self.sample_rate,
                blocksize=self.chunk_size,
                dtype=AUDIO_FORMAT
            )
            self.stream.start()
            
            # ÎÖπÏùå ÏõåÏª§ Ïä§Î†àÎìú ÏãúÏûë
            self.recording_thread = threading.Thread(
                target=self._recording_worker, 
                name="AudioRecordingWorker"
            )
            self.recording_thread.start()
            
            self.is_recording = True
            logger.info("Ïò§ÎîîÏò§ ÎÖπÏùåÏù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏãúÏûëÎêòÏóàÏäµÎãàÎã§")
            return True
            
        except Exception as e:
            logger.error(f"ÎÖπÏùå ÏãúÏûë Ïã§Ìå®: {e}")
            self._cleanup_recording_resources()
            return False
    
    def _reset_recording_state(self) -> None:
        """ÏÉà ÎÖπÏùåÏùÑ ÏúÑÌï¥ ÎÖπÏùå ÏÉÅÌÉú Î¶¨ÏÖã"""
        self.recorded_frames = []
        self.audio_queue = queue.Queue()
        self.is_recording = False
    
    def _recording_worker(self) -> None:
        """Ï†ÅÏ†àÌïú ÏóêÎü¨ Ï≤òÎ¶¨Î•º Ìè¨Ìï®Ìïú ÎÖπÏùå ÏõåÏª§ Ïä§Î†àÎìú"""
        logger.debug("ÎÖπÏùå ÏõåÏª§ Ïä§Î†àÎìúÍ∞Ä ÏãúÏûëÎêòÏóàÏäµÎãàÎã§")
        
        while self.is_recording:
            try:
                # ÌÉÄÏûÑÏïÑÏõÉÍ≥º Ìï®Íªò Ïò§ÎîîÏò§ Ï≤≠ÌÅ¨ Í∞ÄÏ†∏Ïò§Í∏∞
                audio_chunk = self.audio_queue.get(timeout=QUEUE_TIMEOUT)
                self.recorded_frames.append(audio_chunk)
            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"ÎÖπÏùå ÏõåÏª§ÏóêÏÑú Ïò§Î•ò Î∞úÏÉù: {e}")
                break
                
        logger.debug("ÎÖπÏùå ÏõåÏª§ Ïä§Î†àÎìúÍ∞Ä Ï¢ÖÎ£åÎêòÏóàÏäµÎãàÎã§")
    
    def stop_recording(self) -> Optional[str]:
        """Ï†ÅÏ†àÌïú Ï†ïÎ¶¨ÏôÄ Ìï®Íªò ÎÖπÏùå Ï§ëÏßÄ Î∞è Ïò§ÎîîÏò§ ÌååÏùº Ï†ÄÏû•"""
        if not self.is_recording:
            logger.warning("ÏßÑÌñâ Ï§ëÏù∏ ÎÖπÏùåÏù¥ ÏóÜÏäµÎãàÎã§")
            return None
            
        logger.info("Ïò§ÎîîÏò§ ÎÖπÏùåÏùÑ Ï§ëÏßÄÌï©ÎãàÎã§...")
        self.is_recording = False
        
        # Ïò§ÎîîÏò§ Ïä§Ìä∏Î¶º Ï§ëÏßÄ Î∞è Ï†ïÎ¶¨
        self._cleanup_recording_resources()
        
        # ÎÖπÏùåÎêú Îç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏
        if not self.recorded_frames:
            logger.warning("ÎÖπÏùåÎêú Ïò§ÎîîÏò§ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§")
            return None
        
        # Ïò§ÎîîÏò§ Îç∞Ïù¥ÌÑ∞ Í≤∞Ìï©
        try:
            audio_data = np.concatenate(self.recorded_frames, axis=0)
            logger.info(f"{len(self.recorded_frames)}Í∞úÏùò Ïò§ÎîîÏò§ ÌîÑÎ†àÏûÑ Í≤∞Ìï© ÏôÑÎ£å")
        except Exception as e:
            logger.error(f"Ïò§ÎîîÏò§ Îç∞Ïù¥ÌÑ∞ Í≤∞Ìï© Ïã§Ìå®: {e}")
            return None
        
        # Ïò§ÎîîÏò§ ÌååÏùº Ï†ÄÏû•
        return self._save_audio_file(audio_data)
    
    def _cleanup_recording_resources(self) -> None:
        """ÎÖπÏùå Î¶¨ÏÜåÏä§Î•º ÏïàÏ†ÑÌïòÍ≤å Ï†ïÎ¶¨"""
        # Ïò§ÎîîÏò§ Ïä§Ìä∏Î¶º Ï§ëÏßÄ
        if self.stream is not None:
            try:
                self.stream.stop()
                self.stream.close()
                logger.debug("Ïò§ÎîîÏò§ Ïä§Ìä∏Î¶ºÏù¥ Ï§ëÏßÄÎêòÍ≥† Îã´ÌòîÏäµÎãàÎã§")
            except Exception as e:
                logger.error(f"Ïò§ÎîîÏò§ Ïä§Ìä∏Î¶º Ï§ëÏßÄ Ïò§Î•ò: {e}")
            finally:
                self.stream = None
        
        # ÎÖπÏùå Ïä§Î†àÎìúÍ∞Ä Ï¢ÖÎ£åÎê† ÎïåÍπåÏßÄ ÎåÄÍ∏∞
        if self.recording_thread is not None:
            try:
                self.recording_thread.join(timeout=RECORDING_TIMEOUT)
                if self.recording_thread.is_alive():
                    logger.warning("ÎÖπÏùå Ïä§Î†àÎìúÍ∞Ä ÌÉÄÏûÑÏïÑÏõÉ ÎÇ¥Ïóê Ï¢ÖÎ£åÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§")
                logger.debug("ÎÖπÏùå Ïä§Î†àÎìú Ï¢ÖÎ£å ÎåÄÍ∏∞ ÏôÑÎ£å")
            except Exception as e:
                logger.error(f"ÎÖπÏùå Ïä§Î†àÎìú Ï¢ÖÎ£å ÎåÄÍ∏∞ Ïò§Î•ò: {e}")
            finally:
                self.recording_thread = None
    
    def _save_audio_file(self, audio_data: np.ndarray) -> Optional[str]:
        """ÏóêÎü¨ Ï≤òÎ¶¨ÏôÄ Ìï®Íªò Ïò§ÎîîÏò§ Îç∞Ïù¥ÌÑ∞Î•º ÌååÏùºÎ°ú Ï†ÄÏû•"""
        timestamp = datetime.now().strftime(FILE_TIMESTAMP_FORMAT)
        filename = f"recording_{timestamp}.wav"
        filepath = os.path.join(TEMP_AUDIO_DIR, filename)
        
        try:
            sf.write(filepath, audio_data, self.sample_rate)
            logger.info(f"Ïò§ÎîîÏò§ ÌååÏùº Ï†ÄÏû• ÏôÑÎ£å: {filepath}")
            return filepath
        except Exception as e:
            logger.error(f"Ïò§ÎîîÏò§ ÌååÏùº Ï†ÄÏû• Ïã§Ìå®: {e}")
            return None


class TranscriptionFormatter:
    """Ï†ÑÏÇ¨ Í≤∞Í≥ºÏùò Ìè¨Îß§ÌåÖÍ≥º Î∂ÑÏÑùÏùÑ Ï≤òÎ¶¨"""
    
    @staticmethod
    def create_error_result(message: str, audio_file: str = "") -> Dict[str, Any]:
        """ÌëúÏ§ÄÌôîÎêú ÏóêÎü¨ Í≤∞Í≥º ÏÉùÏÑ±"""
        return {
            "status": ProcessingStatus.ERROR.value,
            "message": message,
            "audio_file": audio_file,
            "timestamp": datetime.now().isoformat()
        }
    
    @staticmethod
    def format_time_display(seconds: float) -> str:
        """Ï¥àÎ•º MMÎ∂Ñ SSÏ¥à ÌëúÏãú ÌòïÏãùÏúºÎ°ú Ìè¨Îß§ÌåÖ"""
        minutes = int(seconds // 60)
        seconds_remainder = int(seconds % 60)
        return TIME_DISPLAY_FORMAT.format(minutes=minutes, seconds=seconds_remainder)
    
    @staticmethod
    def format_timestamp_display(seconds: float) -> str:
        """Ï¥àÎ•º MM:SS ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ ÌòïÏãùÏúºÎ°ú Ìè¨Îß§ÌåÖ"""
        minutes = int(seconds // 60)
        seconds_remainder = int(seconds % 60)
        return f"{minutes:02d}:{seconds_remainder:02d}"
    
    @staticmethod
    def create_speaker_mapping(
        unique_labels: List[str], 
        participants_info: Optional[Dict[str, Dict[str, str]]]
    ) -> Dict[str, str]:
        """ÌôîÏûê ÎùºÎ≤®ÏóêÏÑú ÌëúÏãú Ïù¥Î¶ÑÏúºÎ°úÏùò Îß§Ìïë ÏÉùÏÑ±"""
        speaker_mapping = {}
        
        for i, label in enumerate(sorted(unique_labels)):
            if participants_info and label in participants_info:
                speaker_name = participants_info[label]["name"]
            else:
                speaker_name = chr(SPEAKER_ALPHABET_START + i)  # A, B, C...
            speaker_mapping[label] = speaker_name
            
        return speaker_mapping
    
    @staticmethod
    def calculate_speaker_times(utterances: List[Any]) -> Dict[str, float]:
        """Í∞Å ÌôîÏûêÏùò Ï¥ù Î∞úÏñ∏ ÏãúÍ∞ÑÏùÑ Î∞ÄÎ¶¨Ï¥à Îã®ÏúÑÎ°ú Í≥ÑÏÇ∞"""
        speaker_times = {}
        
        for utterance in utterances:
            speaker = utterance.speaker
            duration = utterance.end - utterance.start  # Î∞ÄÎ¶¨Ï¥à
            
            speaker_times[speaker] = speaker_times.get(speaker, 0) + duration
            
        return speaker_times
    
    @staticmethod
    def create_speaker_time_info(
        speaker_times: Dict[str, float], 
        speaker_mapping: Dict[str, str],
        total_time_seconds: float
    ) -> Dict[str, Dict[str, Union[float, str]]]:
        """ÏÉÅÏÑ∏Ìïú ÌôîÏûê ÏãúÍ∞Ñ Ï†ïÎ≥¥ ÏÉùÏÑ±"""
        speaker_time_info = {}
        
        for speaker, time_ms in speaker_times.items():
            time_seconds = time_ms / 1000
            percentage = (time_seconds / total_time_seconds * 100) if total_time_seconds > 0 else 0
            speaker_name = speaker_mapping.get(speaker, "Ï∞∏ÏÑùÏûê")
            
            speaker_time_info[speaker_name] = {
                "total_seconds": round(time_seconds, 2),
                "formatted_time": TranscriptionFormatter.format_time_display(time_seconds),
                "percentage": round(percentage, 1)
            }
            
        return speaker_time_info
    
    @staticmethod
    def create_utterances_info(
        utterances: List[Any], 
        speaker_mapping: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """ÌîÑÎ°†Ìä∏ÏóîÎìúÎ•º ÏúÑÌïú ÏÉÅÏÑ∏Ìïú Î∞úÌôî Ï†ïÎ≥¥ ÏÉùÏÑ±"""
        utterances_info = []
        
        for utterance in utterances:
            speaker_name = speaker_mapping.get(utterance.speaker, "Ï∞∏ÏÑùÏûê")
            start_seconds = utterance.start / 1000  # Î∞ÄÎ¶¨Ï¥àÏóêÏÑú Ï¥àÎ°ú
            end_seconds = utterance.end / 1000
            duration = end_seconds - start_seconds
            
            utterances_info.append({
                "speaker": speaker_name,
                "start_time": round(start_seconds, 1),
                "end_time": round(end_seconds, 1),
                "duration": round(duration, 1),
                "text": utterance.text,
                "formatted_start": TranscriptionFormatter.format_timestamp_display(start_seconds)
            })
            
        return utterances_info
    
    @staticmethod
    def create_participants_info(
        speaker_mapping: Dict[str, str], 
        participants_info: Optional[Dict[str, Dict[str, str]]]
    ) -> List[Dict[str, str]]:
        """Ï∞∏ÏÑùÏûê Ï†ïÎ≥¥ Î¶¨Ïä§Ìä∏ ÏÉùÏÑ±"""
        participants = []
        
        for label, speaker_name in speaker_mapping.items():
            role = "Ï∞∏ÏÑùÏûê"
            if participants_info and label in participants_info:
                role = participants_info[label].get("role", "Ï∞∏ÏÑùÏûê")
            
            participants.append({
                "id": label,
                "name": speaker_name,
                "role": role
            })
            
        return participants
    
    @staticmethod
    def log_speaker_analysis(speaker_time_info: Dict[str, Dict[str, Union[float, str]]], total_time_seconds: float) -> None:
        """ÌôîÏûê ÏãúÍ∞Ñ Î∂ÑÏÑùÏùÑ ÏΩòÏÜîÏóê Î°úÍ∑∏ Ï∂úÎ†•"""
        logger.info("\nüìä ÌôîÏûêÎ≥Ñ Î∞úÏñ∏ ÏãúÍ∞Ñ Î∂ÑÏÑù:")
        logger.info("-" * 40)
        
        for speaker_name, info in speaker_time_info.items():
            logger.info(
                f"{speaker_name}: {info['formatted_time']} ({info['percentage']}%)"
            )
        
        total_display = TranscriptionFormatter.format_time_display(total_time_seconds)
        logger.info(f"Ï†ÑÏ≤¥ Î∞úÏñ∏ ÏãúÍ∞Ñ: {total_display}")
        logger.info("-" * 40)


class STTProcessor:
    """Ìè¨Í¥ÑÏ†ÅÏù∏ ÏóêÎü¨ Ï≤òÎ¶¨Î•º Ìè¨Ìï®Ìïú AssemblyAI Í∏∞Î∞ò ÏùåÏÑ±-ÌÖçÏä§Ìä∏ Ï≤òÎ¶¨"""

    def __init__(self, api_key: Optional[str] = None) -> None:
        """API ÌÇ§ Í≤ÄÏ¶ùÍ≥º Ìï®Íªò STTProcessor Ï¥àÍ∏∞Ìôî"""
        self.api_key = self._resolve_api_key(api_key)
        self._configure_assemblyai()
        
        logger.info("STTProcessorÍ∞Ä ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Ï¥àÍ∏∞ÌôîÎêòÏóàÏäµÎãàÎã§")
    
    def _resolve_api_key(self, api_key: Optional[str]) -> str:
        """Í≤ÄÏ¶ùÍ≥º Ìï®Íªò Ïó¨Îü¨ ÏÜåÏä§ÏóêÏÑú API ÌÇ§ Ìï¥Í≤∞"""
        resolved_key = api_key or ASSEMBLYAI_API_KEY or os.getenv("ASSEMBLYAI_API_KEY")
        
        if not resolved_key:
            raise ValueError(
                "AssemblyAI API key is required. Provide via parameter, "
                "config.py, or ASSEMBLYAI_API_KEY environment variable."
            )
            
        return resolved_key
    
    def _configure_assemblyai(self) -> None:
        """AssemblyAI ÏÑ§Ï†ï Íµ¨ÏÑ±"""
        try:
            aai.settings.api_key = self.api_key
            logger.debug("AssemblyAIÍ∞Ä ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Íµ¨ÏÑ±ÎêòÏóàÏäµÎãàÎã§")
        except Exception as e:
            logger.error(f"AssemblyAI Íµ¨ÏÑ± Ïã§Ìå®: {e}")
            raise

    def transcribe_audio(
        self, 
        audio_file: str, 
        participants_info: Optional[Dict[str, Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        Transcribe audio file using AssemblyAI and format results
        
        Args:
            audio_file: Path to audio file to process
            participants_info: Speaker information dict (e.g. {"A": {"name": "ÍπÄÏ§ÄÌù¨", "role": "ÌåÄÏõê"}})
            
        Returns:
            Structured transcription result dictionary
        """
        logger.info(f"Ï†ÑÏÇ¨ ÏãúÏûë: {audio_file}")
        
        # ÏûÖÎ†• ÌååÏùº Í≤ÄÏ¶ù
        if not os.path.exists(audio_file):
            error_msg = f"Audio file not found: {audio_file}"
            logger.error(error_msg)
            return TranscriptionFormatter.create_error_result(error_msg, audio_file)

        try:
            # Ï†ÑÏÇ¨ Íµ¨ÏÑ± ÏÉùÏÑ±
            config = self._create_transcription_config()
            
            # Ï†ÑÏÇ¨ ÏàòÌñâ
            transcript = self._perform_transcription(audio_file, config)
            
            if transcript is None:
                error_msg = "Transcription failed"
                logger.error(error_msg)
                return TranscriptionFormatter.create_error_result(error_msg, audio_file)
            
            # Ï†ÑÏÇ¨ Ïò§Î•ò ÌôïÏù∏
            if transcript.status == aai.TranscriptStatus.error:
                error_msg = f"Transcription failed: {transcript.error}"
                logger.error(error_msg)
                return TranscriptionFormatter.create_error_result(error_msg, audio_file)

            # Í≤∞Í≥º Ìè¨Îß§ÌåÖ
            formatted_result = self._format_transcription_result(transcript, audio_file, participants_info)

            # Í≤∞Í≥º Ï†ÄÏû•
            self._save_transcription_result(formatted_result)
            
            logger.info("Ï†ÑÏÇ¨Í∞Ä ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§")
            return formatted_result

        except Exception as e:
            error_msg = f"Transcription processing error: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return TranscriptionFormatter.create_error_result(error_msg, audio_file)
    
    def _create_gemini_formatted_transcript(
        self, 
        utterances: List[Any],
        speaker_mapping: Dict[str, str], 
        speaker_time_info: Dict[str, Dict[str, Union[float, str]]],
        total_time_seconds: float,
        participants_info: Optional[Dict[str, Dict[str, str]]] = None
    ) -> str:
        """Gemini Î™®Îç∏Ïóê ÏµúÏ†ÅÌôîÎêú Ï†ÑÏÇ¨ ÌÖçÏä§Ìä∏ ÌòïÏãù ÏÉùÏÑ±"""
        lines = []
        
        # Î™ÖÌôïÌïú Íµ¨Î∂ÑÏûêÎ°ú ÏãúÏûë
        lines.append("## 1:1 ÌöåÏùò Ï†ÑÏÇ¨ ÎÇ¥Ïö©")
        lines.append("")
        
        # Ï∞∏ÏÑùÏûê Ï†ïÎ≥¥Îßå Ìè¨Ìï® (Ïó≠Ìï†Îßå ÌëúÏãú)
        if participants_info:
            lines.append("### Ï∞∏ÏÑùÏûê Ï†ïÎ≥¥")
            for speaker_name in speaker_time_info.keys():
                role_info = ""
                for label, info in participants_info.items():
                    if info.get("name") == speaker_name:
                        role_info = f": {info.get('role', 'Ï∞∏ÏÑùÏûê')}"
                        break
                lines.append(f"- **{speaker_name}{role_info}**")
            lines.append("")
        
        # ÎåÄÌôî ÎÇ¥Ïö©
        lines.append("### ÎåÄÌôî ÎÇ¥Ïö©")
        for utterance in utterances:
            speaker_name = speaker_mapping.get(utterance.speaker, 'Ï∞∏ÏÑùÏûê')
            start_time = TranscriptionFormatter.format_timestamp_display(utterance.start / 1000)
            # Í∞ÑÏÜåÌôîÎêú ÌòïÏãù: [ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ] ÌôîÏûêÎ™Ö: ÎÇ¥Ïö©
            lines.append(f"[{start_time}] {speaker_name}: {utterance.text}")
        
        return "\n".join(lines)
    
    def _create_transcription_config(self) -> aai.TranscriptionConfig:
        """AssemblyAI Ï†ÑÏÇ¨ Íµ¨ÏÑ± ÏÉùÏÑ±"""
        try:
            config = aai.TranscriptionConfig(
                language_code=ASSEMBLYAI_LANGUAGE,
                punctuate=ASSEMBLYAI_PUNCTUATE,
                format_text=ASSEMBLYAI_FORMAT_TEXT,
                disfluencies=ASSEMBLYAI_DISFLUENCIES,
                speaker_labels=ASSEMBLYAI_SPEAKER_LABELS,
                language_detection=ASSEMBLYAI_LANGUAGE_DETECTION,
                speakers_expected=ASSEMBLYAI_SPEAKERS_EXPECTED
            )

            # Íµ¨ÏÑ±Îêú Í≤ΩÏö∞ Îã®Ïñ¥ Î∂ÄÏä§Ìä∏ Ï∂îÍ∞Ä
            if ASSEMBLYAI_WORD_BOOST:
                config.word_boost = ASSEMBLYAI_WORD_BOOST
                config.boost_param = ASSEMBLYAI_BOOST_PARAM
                
            logger.debug("Ï†ÑÏÇ¨ Íµ¨ÏÑ±Ïù¥ ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§")
            return config
            
        except Exception as e:
            logger.error(f"Ï†ÑÏÇ¨ Íµ¨ÏÑ± ÏÉùÏÑ± Ïã§Ìå®: {e}")
            raise
    
    def _perform_transcription(self, audio_file: str, config: aai.TranscriptionConfig) -> Optional[aai.Transcript]:
        """ÏÉÅÌÉú Ìè¥ÎßÅÍ≥º Ìï®Íªò Ï†ÑÏÇ¨ ÏàòÌñâ"""
        try:
            transcriber = aai.Transcriber(config=config)
            transcript = transcriber.transcribe(audio_file)
            
            logger.info("Ï†ÑÏÇ¨ ÏôÑÎ£åÎ•º Í∏∞Îã§Î¶¨Í≥† ÏûàÏäµÎãàÎã§...")
            
            # Ï†ÅÏ†àÌïú ÏóêÎü¨ Ï≤òÎ¶¨ÏôÄ Ìï®Íªò ÏôÑÎ£å Ïó¨Î∂Ä Ìè¥ÎßÅ
            while transcript.status not in [aai.TranscriptStatus.completed, aai.TranscriptStatus.error]:
                logger.debug(f"Ï†ÑÏÇ¨ ÏÉÅÌÉú: {transcript.status}")
                time.sleep(TRANSCRIPT_POLL_INTERVAL)
                transcript = transcript.get()
                
            logger.info(f"Ï†ÑÏÇ¨ ÏôÑÎ£å, ÏÉÅÌÉú: {transcript.status}")
            return transcript
            
        except Exception as e:
            logger.error(f"Ï†ÑÏÇ¨ Ïã§Ìå®: {e}")
            return None

    def _format_transcription_result(
        self, 
        transcript: aai.Transcript, 
        audio_file: str, 
        participants_info: Optional[Dict[str, Dict[str, str]]]
    ) -> Dict[str, Any]:
        """AssemblyAI Ï†ÑÏÇ¨ Í≤∞Í≥ºÎ•º Íµ¨Ï°∞ÌôîÎêú JSONÏúºÎ°ú Ìè¨Îß§ÌåÖ"""
        
        # Î∞úÌôîÍ∞Ä ÏóÜÎäî Í≤ΩÏö∞ Ï≤òÎ¶¨
        if not hasattr(transcript, 'utterances') or not transcript.utterances:
            logger.warning("Ï†ÑÏÇ¨ÏóêÏÑú Î∞úÌôîÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§")
            return {
                "transcript": "Ï†ÑÏÇ¨Îêú ÎÇ¥Ïö©Ïù¥ ÏóÜÏäµÎãàÎã§.",
                "status": ProcessingStatus.SUCCESS_NO_UTTERANCES.value,
                "timestamp": datetime.now().isoformat(),
                "speaker_times": {},
                "utterances": [],
                "participants": []
            }

        # Í≥†Ïú†Ìïú ÌôîÏûêÎì§ÏùÑ Ï∂îÏ∂úÌïòÍ≥† Îß§Ìïë ÏÉùÏÑ±
        unique_labels = list(set(u.speaker for u in transcript.utterances))
        speaker_mapping = TranscriptionFormatter.create_speaker_mapping(unique_labels, participants_info)
        
        # ÌôîÏûê ÏãúÍ∞Ñ Í≥ÑÏÇ∞
        speaker_times = TranscriptionFormatter.calculate_speaker_times(transcript.utterances)
        
        # ÌÅ¥Î°úÎ∞î ÎÖ∏Ìä∏ Î∞©Ïãù: ÏàúÏàò Î∞úÏñ∏ ÏãúÍ∞Ñ Ìï©Í≥ÑÎ•º Í∏∞Ï§ÄÏúºÎ°ú Ï†êÏú†Ïú® Í≥ÑÏÇ∞ (Ïπ®Î¨µ Íµ¨Í∞Ñ Ï†úÏô∏)
        total_speaking_time_seconds = sum(speaker_times.values()) / 1000  # Î∞ÄÎ¶¨Ï¥àÏóêÏÑú Ï¥àÎ°ú Î≥ÄÌôò
        
        # Ïã§Ï†ú ÌöåÏùò Ï†ÑÏ≤¥ ÏãúÍ∞Ñ Í≥ÑÏÇ∞ (ÌÜµÍ≥ÑÏö©)
        if transcript.utterances:
            meeting_start = min(u.start for u in transcript.utterances)
            meeting_end = max(u.end for u in transcript.utterances)
            total_meeting_duration_seconds = (meeting_end - meeting_start) / 1000  # Î∞ÄÎ¶¨Ï¥àÏóêÏÑú Ï¥àÎ°ú
        else:
            total_meeting_duration_seconds = 0
        
        # ÏÉÅÏÑ∏Ìïú ÌôîÏûê ÏãúÍ∞Ñ Ï†ïÎ≥¥ ÏÉùÏÑ± (ÌÅ¥Î°úÎ∞î ÎÖ∏Ìä∏ Î∞©Ïãù: ÏàúÏàò Î∞úÏñ∏ ÏãúÍ∞Ñ Í∏∞Ï§Ä)
        speaker_time_info = TranscriptionFormatter.create_speaker_time_info(
            speaker_times, speaker_mapping, total_speaking_time_seconds
        )
        
        # ÌôîÏûê Î∂ÑÏÑù Î°úÍ∑∏ Ï∂úÎ†•
        TranscriptionFormatter.log_speaker_analysis(speaker_time_info, total_speaking_time_seconds)
        
        # GeminiÏö© Íµ¨Ï°∞ÌôîÎêú Ï†ÑÏÇ¨ ÌÖçÏä§Ìä∏ ÏÉùÏÑ±
        gemini_formatted_transcript = self._create_gemini_formatted_transcript(
            transcript.utterances, speaker_mapping, speaker_time_info, total_speaking_time_seconds, participants_info
        )
        
        # ÏÉÅÏÑ∏Ìïú Î∞úÌôî Ï†ïÎ≥¥ ÏÉùÏÑ±
        utterances = TranscriptionFormatter.create_utterances_info(transcript.utterances, speaker_mapping)
        
        # Ï∞∏ÏÑùÏûê Ï†ïÎ≥¥ ÏÉùÏÑ±
        participants = TranscriptionFormatter.create_participants_info(speaker_mapping, participants_info)

        return {
            # Gemini Î™®Îç∏ ÏµúÏ†ÅÌôîÎêú Ï†ÑÏÇ¨ ÎÇ¥Ïö©
            "transcript": gemini_formatted_transcript,
            "status": ProcessingStatus.SUCCESS.value,
            "timestamp": datetime.now().isoformat(),
            
            # ÌôîÏûê ÌÜµÍ≥Ñ Ï†ïÎ≥¥
            "speaker_times": speaker_time_info,
            "total_duration_seconds": round(total_meeting_duration_seconds, 2),
            "total_speaking_time_seconds": round(total_speaking_time_seconds, 2),
            
            # ÌîÑÎ°†Ìä∏ÏóîÎìúÏö© ÏÉÅÏÑ∏ Ï†ïÎ≥¥ (ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ Ìè¨Ìï®)
            "utterances": utterances,
            "participants": participants
        }

    def _save_transcription_result(self, result: Dict[str, Any]) -> None:
        """ÏóêÎü¨ Ï≤òÎ¶¨ÏôÄ Ìï®Íªò Ï†ÑÏÇ¨ Í≤∞Í≥ºÎ•º JSON ÌååÏùºÎ°ú Ï†ÄÏû•"""
        try:
            timestamp = datetime.now().strftime(FILE_TIMESTAMP_FORMAT)
            json_file_path = os.path.join(OUTPUT_DIR, f"transcript_{timestamp}.json")
            
            with open(json_file_path, "w", encoding="utf-8") as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
                
            logger.info(f"üíæ Transcription result saved: {json_file_path}")
            
        except Exception as e:
            logger.error(f"Ï†ÑÏÇ¨ Í≤∞Í≥º Ï†ÄÏû• Ïã§Ìå®: {e}")


class AudioProcessor:
    """Ïò§ÎîîÏò§ ÎÖπÏùåÍ≥º Ï†ÑÏÇ¨ Ï≤òÎ¶¨Î•º ÌÜµÌï©Ìïú Í≥†ÏàòÏ§Ä Ïù∏ÌÑ∞ÌéòÏù¥Ïä§"""
    
    def __init__(self, assemblyai_api_key: Optional[str] = None) -> None:
        """
        Initialize AudioProcessor with recording and transcription capabilities
        
        Args:
            assemblyai_api_key: AssemblyAI API key for transcription service
        """
        try:
            self.recorder = AudioRecorder()
            self.stt_processor = STTProcessor(api_key=assemblyai_api_key)
            logger.info("AudioProcessorÍ∞Ä ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Ï¥àÍ∏∞ÌôîÎêòÏóàÏäµÎãàÎã§")
        except Exception as e:
            logger.error(f"AudioProcessor Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            raise
    
    def start_recording(self) -> bool:
        """Ïò§ÎîîÏò§ ÎÖπÏùå ÏãúÏûë"""
        logger.info("Ïò§ÎîîÏò§ ÎÖπÏùåÏùÑ ÏãúÏûëÌï©ÎãàÎã§...")
        return self.recorder.start_recording()
    
    def stop_recording_and_transcribe(
        self, 
        participants_info: Optional[Dict[str, Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        Stop recording and perform transcription with cleanup
        
        Args:
            participants_info: Speaker information dictionary
            
        Returns:
            Transcription result dictionary
        """
        logger.info("ÎÖπÏùåÏùÑ Ï§ëÏßÄÌïòÍ≥† Ï†ÑÏÇ¨Î•º ÏãúÏûëÌï©ÎãàÎã§...")
        
        # ÎÖπÏùå Ï§ëÏßÄÌïòÍ≥† ÌååÏùº Í≤ΩÎ°ú Í∞ÄÏ†∏Ïò§Í∏∞
        audio_file = self.recorder.stop_recording()
        
        if not audio_file:
            error_msg = "No recorded audio file available"
            logger.warning(error_msg)
            return TranscriptionFormatter.create_error_result(error_msg)
        
        logger.info(f"ÎÖπÏùå ÏôÑÎ£å: {audio_file}")
        
        # Ï†ÑÏÇ¨ ÏàòÌñâ
        logger.info("Ï†ÑÏÇ¨ ÏãúÏûë...")
        transcription_result = self.stt_processor.transcribe_audio(audio_file, participants_info)
        
        # ÏûÑÏãú ÌååÏùº Ï†ïÎ¶¨
        self._cleanup_temp_file(audio_file)
        
        return transcription_result
    
    def _cleanup_temp_file(self, audio_file: str) -> None:
        """ÏóêÎü¨ Ï≤òÎ¶¨ÏôÄ Ìï®Íªò ÏûÑÏãú Ïò§ÎîîÏò§ ÌååÏùº Ï†ïÎ¶¨"""
        try:
            os.remove(audio_file)
            logger.info(f"ÏûÑÏãú ÌååÏùº Ï†ïÎ¶¨ ÏôÑÎ£å: {audio_file}")
        except Exception as e:
            logger.warning(f"ÏûÑÏãú ÌååÏùº {audio_file} Ï†ïÎ¶¨ Ïã§Ìå®: {e}")

    def transcribe_existing_file(
        self, 
        audio_file: str, 
        participants_info: Optional[Dict[str, Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        Transcribe existing audio file
        
        Args:
            audio_file: Path to audio file
            participants_info: Speaker information dictionary
            
        Returns:
            Transcription result dictionary
        """
        logger.info(f"Í∏∞Ï°¥ ÌååÏùº Ï†ÑÏÇ¨: {audio_file}")
        return self.stt_processor.transcribe_audio(audio_file, participants_info)