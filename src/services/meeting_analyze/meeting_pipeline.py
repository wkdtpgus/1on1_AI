from typing import Dict, List, Any
from langgraph.graph import StateGraph, END
import assemblyai as aai
from supabase import Client
import json
import logging
from datetime import datetime

from src.config.config import ASSEMBLYAI_SPEAKERS_EXPECTED
from src.utils.model import GeminiMeetingAnalyzer, AssemblyAIProcessor
from src.utils.stt_schemas import MeetingPipelineState, MeetingAnalysis
from src.prompts.stt_generation.meeting_analysis_prompts import SYSTEM_PROMPT, USER_PROMPT
from langchain.prompts import PromptTemplate
from langchain_core.prompts import ChatPromptTemplate

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger("meeting_pipeline")

def retrieve_from_supabase(state: MeetingPipelineState) -> MeetingPipelineState:
    """Supabaseì—ì„œ íŒŒì¼ ì¡°íšŒ ë° URL ìƒì„±"""
    logger.info(f"ğŸ” Supabase íŒŒì¼ ì¡°íšŒ ì‹œì‘: {state['file_id']}")
    
    try:
        # íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ supabase í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
        # ì´ ë¶€ë¶„ì€ MeetingPipeline í´ë˜ìŠ¤ì—ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤
        state["status"] = "retrieving_file"
        
        # ì‹¤ì œ íŒŒì¼ ê²€ìƒ‰ì€ MeetingPipeline.runì—ì„œ ìˆ˜í–‰í•˜ê³ 
        # ê²°ê³¼ë¥¼ stateì— ë¯¸ë¦¬ ì„¤ì •í•©ë‹ˆë‹¤
        logger.info(f"âœ… íŒŒì¼ ì¡°íšŒ ì™„ë£Œ: {state.get('file_path', 'N/A')}")
        
    except Exception as e:
        error_msg = f"Supabase íŒŒì¼ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        logger.error(error_msg)
        state["errors"].append(error_msg)
        state["status"] = "failed"
    
    return state


def process_with_assemblyai(state: MeetingPipelineState) -> MeetingPipelineState:
    """AssemblyAIë¡œ STT ì²˜ë¦¬"""
    logger.info("ğŸ™ï¸ STT ì²˜ë¦¬ ì‹œì‘")
    
    try:
        state["status"] = "transcribing"
        
        if not state.get("file_url"):
            raise ValueError("íŒŒì¼ URLì´ ì—†ìŠµë‹ˆë‹¤")
        
        # AssemblyAI ì„¤ì • ìƒì„±
        processor = AssemblyAIProcessor()
        config = processor.create_config(expected_speakers=ASSEMBLYAI_SPEAKERS_EXPECTED)
        
        # STT ì²˜ë¦¬
        transcriber = aai.Transcriber(config=config)
        transcript = transcriber.transcribe(state["file_url"])
        
        if transcript.status == aai.TranscriptStatus.error:
            raise Exception(f"STT ì²˜ë¦¬ ì‹¤íŒ¨: {transcript.error}")
        
        # í™”ì í†µê³„ ê³„ì‚° (ì›ë³¸ transcript ì‚¬ìš©)
        speaker_stats = {}
        if transcript.utterances:
            for utterance in transcript.utterances:
                speaker = utterance.speaker or 'Unknown'
                if speaker not in speaker_stats:
                    speaker_stats[speaker] = {'word_count': 0, 'duration': 0}
                speaker_stats[speaker]['word_count'] += len(utterance.text.split()) if utterance.text else 0
                speaker_stats[speaker]['duration'] += (utterance.end or 0) - (utterance.start or 0)
        
        transcript_dict = {
            "utterances": [
                {
                    "speaker": utterance.speaker,
                    "text": utterance.text
                }
                for utterance in transcript.utterances
            ] if transcript.utterances else [],
            "metadata": {
                "file_id": state["file_id"],
                "processed_at": datetime.now().isoformat(),
                "total_duration": transcript.audio_duration
            }
        }
        
        state["transcript"] = transcript_dict
        state["speaker_stats"] = speaker_stats
        
        logger.info("âœ… STT ì²˜ë¦¬ ì™„ë£Œ")
        
    except Exception as e:
        error_msg = f"STT ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
        logger.error(error_msg)
        state["errors"].append(error_msg)
        state["status"] = "failed"
    
    return state


def analyze_with_llm(state: MeetingPipelineState) -> MeetingPipelineState:
    """LLMìœ¼ë¡œ íšŒì˜ ë¶„ì„"""
    logger.info("ğŸ¤– LLM ë¶„ì„ ì‹œì‘")
    
    try:
        state["status"] = "analyzing"
        
        if not state.get("transcript"):
            raise ValueError("ì „ì‚¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # analyzerì—ì„œ LLM ê°€ì ¸ì˜¤ê¸°
        analyzer = analyze_with_llm._analyzer if hasattr(analyze_with_llm, '_analyzer') else None
        if not analyzer:
            raise ValueError("ë¶„ì„ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # í™”ì í†µê³„ ê°„ì†Œí™” (ë°œì–¸ ë¹„ìœ¨ ê³„ì‚°)
        simplified_stats = {}
        speaker_stats = state.get("speaker_stats", {})
        if speaker_stats:
            total_words = sum(stats.get('word_count', 0) for stats in speaker_stats.values())
            for speaker_name, stats in speaker_stats.items():
                word_count = stats.get('word_count', 0)
                percentage = round((word_count / total_words) * 100, 1) if total_words > 0 else 0
                simplified_stats[speaker_name] = percentage
        
        # í”„ë¡¬í”„íŠ¸ ë°ì´í„° ì¤€ë¹„
        user_prompt_template = PromptTemplate(
            input_variables=["transcript", "speaker_stats", "participants", "qa_pairs"],
            template=USER_PROMPT
        )
        
        # JSON ë¬¸ìì—´ ë³€í™˜
        speaker_stats_json = json.dumps(simplified_stats, ensure_ascii=False) if simplified_stats else "{}"
        qa_pairs_json = json.dumps(state.get("qa_data"), ensure_ascii=False) if state.get("qa_data") else "[]"
        participants_json = json.dumps(state.get("participants_info"), ensure_ascii=False) if state.get("participants_info") else "{}"
        
        # í”„ë¡¬í”„íŠ¸ ì²´ì¸ êµ¬ì„±
        prompt = ChatPromptTemplate.from_messages([
            ("system", SYSTEM_PROMPT),
            ("human", user_prompt_template.template)
        ])
        
        # LLMì— í•„ìš”í•œ transcript ë°ì´í„°ë§Œ ì¶”ì¶œ
        transcript_for_llm = [
            {
                "speaker": utterance["speaker"],
                "text": utterance["text"]
            }
            for utterance in state["transcript"].get("utterances", [])
        ]
        
        # ì²´ì¸ ìƒì„± ë° ì‹¤í–‰
        chain = prompt | analyzer.llm.with_structured_output(MeetingAnalysis)
        
        input_data = {
            "transcript": transcript_for_llm,
            "speaker_stats": speaker_stats_json,
            "participants": participants_json,
            "qa_pairs": qa_pairs_json
        }
        
        result = chain.invoke(input_data)
        
        if result is None:
            raise ValueError("1:1 íšŒì˜ ë¶„ì„ ì‹¤íŒ¨")
        
        # LLM ì‘ë‹µ ìƒì„¸ ë¡œê¹…
        logger.info(f"LLM ì‘ë‹µ íƒ€ì…: {type(result)}")
        logger.info(f"LLM ì‘ë‹µ í•„ë“œ: {result.__dict__.keys() if hasattr(result, '__dict__') else 'N/A'}")
        if hasattr(result, 'detailed_discussion'):
            logger.info(f"detailed_discussion ê¸¸ì´: {len(result.detailed_discussion) if result.detailed_discussion else 0}")
            logger.info(f"detailed_discussion ë§ˆì§€ë§‰ 100ì: {result.detailed_discussion[-100:] if result.detailed_discussion else 'N/A'}")
        
        analysis_result = result.model_dump_json(indent=2)
        
        # JSON ê²°ê³¼ íŒŒì‹±
        try:
            analysis_data = json.loads(analysis_result)
        except json.JSONDecodeError:
            analysis_data = {"error": "ë¶„ì„ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨", "raw_result": analysis_result}
        
        state["analysis_result"] = analysis_data
        state["status"] = "completed"
        
        logger.info("âœ… LLM ë¶„ì„ ì™„ë£Œ")
        
    except Exception as e:
        error_msg = f"LLM ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
        logger.error(error_msg)
        state["errors"].append(error_msg)
        state["status"] = "failed"
    
    return state


# =============================================================================
# Pipeline í´ë˜ìŠ¤
# =============================================================================

class MeetingPipeline:
    """1on1 ë¯¸íŒ… ë¶„ì„ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, supabase_client: Client, analyzer: GeminiMeetingAnalyzer):
        """
        Args:
            supabase_client: Supabase í´ë¼ì´ì–¸íŠ¸
            analyzer: íšŒì˜ ë¶„ì„ê¸°
        """
        self.supabase = supabase_client
        self.analyzer = analyzer
        self.workflow = self._build_graph()
        logger.info("MeetingPipeline ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _build_graph(self) -> Any:
        """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±"""
        workflow = StateGraph(MeetingPipelineState)
        
        # analyzerë¥¼ ë…¸ë“œ í•¨ìˆ˜ì— ë°”ì¸ë”©
        analyze_with_llm._analyzer = self.analyzer
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("retrieve", retrieve_from_supabase)
        workflow.add_node("transcribe", process_with_assemblyai)
        workflow.add_node("analyze", analyze_with_llm)
        
        # ì—£ì§€ ì—°ê²°
        workflow.set_entry_point("retrieve")
        workflow.add_edge("retrieve", "transcribe")
        workflow.add_edge("transcribe", "analyze")
        workflow.add_edge("analyze", END)
        
        return workflow.compile()
    
    async def find_file_in_storage(self, bucket_name: str, file_id: str) -> List[Dict]:
        """ì¬ê·€ì ìœ¼ë¡œ ìŠ¤í† ë¦¬ì§€ì—ì„œ íŒŒì¼ì„ ì°¾ëŠ” í•¨ìˆ˜"""
        def search_files(path: str = ""):
            try:
                files = self.supabase.storage.from_(bucket_name).list(path)
                found = []
                
                for file in files:
                    file_path = f"{path}/{file['name']}" if path else file['name']
                    
                    if file.get('id') is not None or '.' in file['name']:
                        if (file['name'] == file_id or 
                            file_path == file_id or 
                            file_id in file['name']):
                            found.append({**file, "full_path": file_path})
                    else:
                        try:
                            sub_found = search_files(file_path)
                            found.extend(sub_found)
                        except:
                            pass
                return found
            except:
                return []
        
        return search_files()
    
    async def run(self, file_id: str, **kwargs) -> Dict:
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info(f"ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘: {file_id}")
        
        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        bucket_name = kwargs.get("bucket_name", "audio-recordings")
        
        # Supabase íŒŒì¼ ì¡°íšŒ (LangGraph ì‹¤í–‰ ì „ì— ìˆ˜í–‰)
        found_files = await self.find_file_in_storage(bucket_name, file_id)
        if not found_files:
            raise ValueError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_id}")
        
        file_info = found_files[0]
        file_path = file_info.get('full_path', file_info['name'])
        file_url = self.supabase.storage.from_(bucket_name).get_public_url(file_path)
        
        initial_state: MeetingPipelineState = {
            "file_id": file_id,
            "bucket_name": bucket_name,
            "qa_data": kwargs.get("qa_data"),
            "participants_info": kwargs.get("participants_info"),
            "file_url": file_url,
            "file_path": file_path,
            "file_metadata": file_info,
            "transcript": None,
            "speaker_stats": None,
            "analysis_result": None,
            "errors": [],
            "status": "pending"
        }
        
        # LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        result = await self.workflow.ainvoke(initial_state)
        
        logger.info(f"âœ… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ: {result['status']}")
        
        return result