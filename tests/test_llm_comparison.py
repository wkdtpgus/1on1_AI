"""
LLM ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
OpenAI GPT vs Google Vertex AI Gemini ì„±ëŠ¥ ë° ê²°ê³¼ í’ˆì§ˆ ë¹„êµ
"""

import os
import sys
import json
from datetime import datetime
from typing import List
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from langchain_google_vertexai import ChatVertexAI
from langchain.schema import HumanMessage
from src.models.multi_llm_analyzer import MultiLLMAnalyzer
from src.prompts.stt_llm_prompts import QA_EXTRACTION_PROMPT
from src.config.config import (
    GOOGLE_CLOUD_PROJECT,
    GOOGLE_CLOUD_LOCATION,
    VERTEX_AI_MODEL,
    VERTEX_AI_TEMPERATURE,
    VERTEX_AI_MAX_TOKENS
)

def load_sample_transcript(file_path: str) -> dict:
    """ì‹¤ì œ ì „ì‚¬ íŒŒì¼ì—ì„œ STT ë°ì´í„° ë¡œë“œ"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
        
        # íŒŒì¼ í˜•ì‹ì— ë”°ë¥¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if file_path.endswith("test_1on1.txt"):
            selected_text = content.strip()
        else:
            selected_text = _extract_transcript_text(content)
        
        return {
            "status": "success",
            "full_text": selected_text,
            "timestamp": "2025-07-28T16:44:07",
            # ë””ë²„ê¹…ìš© ì •ë³´
            "options": _get_debug_info(content)
        }
        
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

def _extract_transcript_text(content: str) -> str:
    """ì „ì‚¬ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    full_transcript = _extract_full_text(content)
    speaker_separated = _extract_speaker_text(content)
    
    # í™”ìë³„ ë°œì–¸ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©
    return speaker_separated.strip() if speaker_separated.strip() else full_transcript.strip()

def _extract_full_text(content: str) -> str:
    """ì „ì²´ í…ìŠ¤íŠ¸ ì„¹ì…˜ ì¶”ì¶œ"""
    lines = content.split('\n')
    full_transcript = ""
    in_full_text = False
    
    for line in lines:
        if "## ì „ì²´ í…ìŠ¤íŠ¸" in line:
            in_full_text = True
            continue
        elif "## í™”ìë³„ ë°œì–¸" in line:
            break
        elif in_full_text and line.strip() and not line.startswith("#"):
            full_transcript += line + " "
    
    return full_transcript

def _extract_speaker_text(content: str) -> str:
    """í™”ìë³„ ë°œì–¸ ì„¹ì…˜ ì¶”ì¶œ"""
    lines = content.split('\n')
    speaker_separated = ""
    in_speaker_section = False
    
    for line in lines:
        if "## í™”ìë³„ ë°œì–¸" in line:
            in_speaker_section = True
            continue
        elif in_speaker_section and line.strip():
            speaker_separated += line + "\n"
    
    return speaker_separated

def _get_debug_info(content: str) -> dict:
    """ë””ë²„ê¹… ì •ë³´ ìƒì„±"""
    full_transcript = _extract_full_text(content)
    speaker_separated = _extract_speaker_text(content)
    
    return {
        "full_transcript_length": len(full_transcript.strip()),
        "speaker_separated_length": len(speaker_separated.strip()), 
        "raw_content_length": len(content)
    }


def print_section(title: str, content: str):
    """ì„¹ì…˜ ì¶œë ¥ í•¨ìˆ˜"""
    print("\n" + "="*80)
    print(f" {title}")
    print("="*80)
    print(content)


def save_comparison_results(results: dict):
    """ë¹„êµ ê²°ê³¼ë¥¼ ê°œë³„ íŒŒì¼ë¡œ ì €ì¥"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # data ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs("data", exist_ok=True)
    
    model_comparison = results.get("model_comparison", {})
    
    # OpenAI ê²°ê³¼ íŒŒì¼ ì €ì¥
    if "openai_analysis" in model_comparison:
        openai_filename = f"openai_gpt_result_{timestamp}.md"
        openai_filepath = os.path.join("data", openai_filename)
        
        with open(openai_filepath, "w", encoding="utf-8") as f:
            f.write("# OpenAI GPT íšŒì˜ ë¶„ì„ ê²°ê³¼\n\n")
            f.write(f"ìƒì„± ì‹œê°„: {timestamp}\n\n")
            f.write("---\n\n")
            f.write(model_comparison["openai_analysis"])
        
        print(f"ğŸ’¾ OpenAI GPT ê²°ê³¼ ì €ì¥: {openai_filepath}")
    
    # Vertex AI ê²°ê³¼ íŒŒì¼ ì €ì¥
    if "vertexai_analysis" in model_comparison:
        vertexai_filename = f"vertexai_gemini_result_{timestamp}.md"
        vertexai_filepath = os.path.join("data", vertexai_filename)
        
        with open(vertexai_filepath, "w", encoding="utf-8") as f:
            f.write("# Google Vertex AI Gemini íšŒì˜ ë¶„ì„ ê²°ê³¼\n\n")
            f.write(f"ìƒì„± ì‹œê°„: {timestamp}\n\n")
            f.write("---\n\n")
            f.write(model_comparison["vertexai_analysis"])
        
        print(f"ğŸ’¾ Vertex AI Gemini ê²°ê³¼ ì €ì¥: {vertexai_filepath}")


def test_qa_extraction(questions: List[str], transcript_file: str):
    """Q&A ì¶”ì¶œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("\nğŸ” Q&A ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print(f"ğŸ“‹ ì§ˆë¬¸ ìˆ˜: {len(questions)}")
    
    # ì „ì‚¬ íŒŒì¼ ë¡œë“œ
    stt_data = load_sample_transcript(transcript_file)
    if not stt_data:
        print("âŒ ì „ì‚¬ íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    transcript = stt_data['full_text']
    print(f"ğŸ“„ ì „ì‚¬ ë‚´ìš© ê¸¸ì´: {len(transcript)}ì")
    
    # Vertex AI ì´ˆê¸°í™”
    try:
        llm = ChatVertexAI(
            project=GOOGLE_CLOUD_PROJECT,
            location=GOOGLE_CLOUD_LOCATION,
            model_name=VERTEX_AI_MODEL,
            temperature=VERTEX_AI_TEMPERATURE,
            max_output_tokens=VERTEX_AI_MAX_TOKENS
        )
        print("âœ… Vertex AI Gemini ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        return
    
    # ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    questions_text = "\n".join([f"{i+1}. {q}" for i, q in enumerate(questions)])
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = QA_EXTRACTION_PROMPT.format(
        questions=questions_text,
        transcript=transcript
    )
    
    # Q&A ì¶”ì¶œ ì‹¤í–‰
    print("\nğŸ”„ Q&A ì¶”ì¶œ ì¤‘...")
    try:
        message = HumanMessage(content=prompt)
        response = llm.invoke([message])
        qa_result = response.content
        
        # ê²°ê³¼ ì¶œë ¥
        print_section("Q&A ì¶”ì¶œ ê²°ê³¼", qa_result)
        
        # ê²°ê³¼ ì €ì¥
        save_qa_result(qa_result, questions, len(transcript))
        
        return qa_result
        
    except Exception as e:
        print(f"âŒ Q&A ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return None


def save_qa_result(qa_result: str, questions: List[str], transcript_length: int):
    """Q&A ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # data ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs("data", exist_ok=True)
    
    # Q&A ê²°ê³¼ íŒŒì¼ ì €ì¥
    qa_filename = f"qa_result_{timestamp}.md"
    qa_filepath = os.path.join("data", qa_filename)
    
    with open(qa_filepath, "w", encoding="utf-8") as f:
        f.write("# Q&A ì§ˆë¬¸ë³„ ë‹µë³€ ì •ë¦¬ ê²°ê³¼\n\n")
        f.write(f"ìƒì„± ì‹œê°„: {timestamp}\n")
        f.write(f"ì´ ì§ˆë¬¸ ìˆ˜: {len(questions)}\n")
        f.write(f"ì „ì‚¬ ë‚´ìš© ê¸¸ì´: {transcript_length}ì\n\n")
        f.write("---\n\n")
        f.write("## ì§ˆë¬¸ ëª©ë¡\n")
        for i, q in enumerate(questions, 1):
            f.write(f"{i}. {q}\n")
        f.write("\n---\n\n")
        f.write("## Q&A ê²°ê³¼\n\n")
        f.write(qa_result)
        f.write(f"\n\n---\n\n")
        f.write("## ë¶„ì„ ì •ë³´\n")
        f.write(f"- ëª¨ë¸: {VERTEX_AI_MODEL}\n")
        f.write(f"- ìƒì„± ì‹œê°„: {timestamp}\n")
    
    print(f"ğŸ’¾ Q&A ê²°ê³¼ ì €ì¥: {qa_filepath}")


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸš€ LLM í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("ì„ íƒí•˜ì„¸ìš”:")
    print("1. LLM ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ (OpenAI vs Gemini)")
    print("2. Q&A ì§ˆë¬¸ë³„ ë‹µë³€ ì¶”ì¶œ í…ŒìŠ¤íŠ¸")
    
    choice = input("\nì„ íƒ (1 ë˜ëŠ” 2): ").strip()
    
    if choice == "1":
        _run_llm_comparison_test()
    elif choice == "2":
        _run_qa_extraction_test()
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1 ë˜ëŠ” 2ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")

def _run_llm_comparison_test():
    """LLM ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    transcript_file = "/Users/kimjoonhee/Documents/Orblit_1on1_AI/test_1on1.txt"
    
    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: OpenAI GPT vs Google Vertex AI Gemini")
    print(f"ğŸ“„ ì „ì‚¬ íŒŒì¼ ë¡œë“œ ì¤‘: {transcript_file}")
    
    stt_data = load_sample_transcript(transcript_file)
    if not stt_data:
        print("âŒ ì „ì‚¬ íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… ì „ì‚¬ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    print(f"   - ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(stt_data['full_text'])}ì")
    
    # MultiLLMAnalyzer ì´ˆê¸°í™”
    print("\nğŸ”§ LLM ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    analyzer = MultiLLMAnalyzer()
    
    # STT ê²°ê³¼ë¥¼ ë‘ ëª¨ë¸ë¡œ ë¶„ì„
    print("\nğŸ”„ STT ê²°ê³¼ ë¶„ì„ ì¤‘...")
    comparison_results = analyzer.analyze_stt_with_comparison(stt_data)
    
    # ê²°ê³¼ ì¶œë ¥
    model_comparison = comparison_results.get("model_comparison", {})
    
    if "error" in model_comparison:
        print(f"âŒ ì˜¤ë¥˜: {model_comparison['error']}")
        return

    # ê²°ê³¼ ì €ì¥
    save_comparison_results(comparison_results)
    print("\nâœ… LLM ë¹„êµ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

def _run_qa_extraction_test():
    """Q&A ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    transcript_file = "/Users/kimjoonhee/Documents/Orblit_1on1_AI/test_1on1.txt"
    
    print("\nğŸ“‹ Q&A ì§ˆë¬¸ë³„ ë‹µë³€ ì¶”ì¶œ í…ŒìŠ¤íŠ¸")
    
    # 1on1 ë¯¸íŒ… ì§ˆë¬¸ë“¤
    sample_questions = [
        "2ë¶„ê¸° ì„±ê³¼ ê²€í† ì™€ 3ë¶„ê¸° ê³„íšì€?",
        "ì–´ë ¤ì› ë˜ ì ì´ë‚˜ ì•„ì‰¬ì› ë˜ ë¶€ë¶„ì€?",
        "ê·¹ë³µí•œ ë°©ë²•",
        "êµ¬ì²´ì ì¸ ì¼ì •ì´ë‚˜ ë§ˆì¼ìŠ¤í†¤",
        "ì˜¬í•´ ì–´ë–¤ ëª©í‘œë¥¼ ì„¸ì› ëŠ”ê°€?",
        "ìµœê·¼ ê°œì¸ì ì¸ ëª©í‘œì™€ ê´€ì‹¬ì‚¬ëŠ”?",
        "ê¶ê¸ˆí•œ ì ì´ë‚˜ ê±´ì˜ì‚¬í•­"
    ]
    
    print("ğŸ“ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ëª©ë¡:")
    for i, q in enumerate(sample_questions, 1):
        print(f"   {i}. {q}")
    
    print("\nğŸ’¡ ì§ˆë¬¸ì„ ìˆ˜ì •í•˜ë ¤ë©´ ì½”ë“œì—ì„œ sample_questions ë¦¬ìŠ¤íŠ¸ë¥¼ í¸ì§‘í•˜ì„¸ìš”.")
    
    # Q&A ì¶”ì¶œ ì‹¤í–‰
    qa_result = test_qa_extraction(sample_questions, transcript_file)
    
    if qa_result:
        print("\nâœ… Q&A ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    else:
        print("\nâŒ Q&A ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")
    


if __name__ == "__main__":
    main()