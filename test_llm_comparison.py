"""
LLM ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
OpenAI GPT vs Google Vertex AI Gemini ì„±ëŠ¥ ë° ê²°ê³¼ í’ˆì§ˆ ë¹„êµ
"""

import os
import json
from datetime import datetime
from typing import List
from langchain_google_vertexai import ChatVertexAI
from langchain.schema import HumanMessage
from src.models.multi_llm_analyzer import MultiLLMAnalyzer
from src.prompts.stt_llm_prompts import QA_EXTRACTION_PROMPT
from src.config.config import (
    GOOGLE_CLOUD_PROJECT,
    GOOGLE_CLOUD_LOCATION,
    VERTEX_AI_MODEL,
    VERTEX_AI_TEMPERATURE,
    VERTEX_AI_MAX_TOKENS
)

def load_sample_transcript(file_path: str) -> dict:
    """ì‹¤ì œ ì „ì‚¬ íŒŒì¼ì—ì„œ STT ë°ì´í„° ë¡œë“œ"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
        
        # 3ê°€ì§€ í˜•íƒœë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        lines = content.split('\n')
        
        # 1. ì „ì²´ ì „ì‚¬ í…ìŠ¤íŠ¸ (ê¸°ì¡´)
        full_transcript = ""
        in_full_text = False
        
        # 2. í™”ìë³„ ë°œì–¸ë§Œ (í™”ì ì •ë³´ í¬í•¨)
        speaker_separated = ""
        in_speaker_section = False
        
        # 3. ì „ì²´ íŒŒì¼ ë‚´ìš© (ì›ë³¸ ê·¸ëŒ€ë¡œ)
        raw_content = content
        
        for line in lines:
            # ì „ì²´ í…ìŠ¤íŠ¸ ì„¹ì…˜ íŒŒì‹±
            if "## ì „ì²´ í…ìŠ¤íŠ¸" in line:
                in_full_text = True
                continue
            elif "## í™”ìë³„ ë°œì–¸" in line:
                in_full_text = False
                in_speaker_section = True
                continue
            elif in_full_text and line.strip() and not line.startswith("#"):
                full_transcript += line + " "
            
            # í™”ìë³„ ë°œì–¸ ì„¹ì…˜ íŒŒì‹±
            elif in_speaker_section and line.strip():
                speaker_separated += line + "\n"
        
        # íŒŒì¼ í˜•ì‹ì— ë”°ë¥¸ í…ìŠ¤íŠ¸ ì„ íƒ
        if file_path.endswith("test_1on1.txt"):
            # test_1on1.txtëŠ” ì¼ë°˜ ëŒ€í™” í˜•ì‹ì´ë¯€ë¡œ ì „ì²´ ë‚´ìš© ì‚¬ìš©
            selected_text = raw_content.strip()
        else:
            # ê¸°ì¡´ ì „ì‚¬ íŒŒì¼ í˜•ì‹ì€ í™”ìë³„ ë°œì–¸ ì‚¬ìš©
            selected_text = speaker_separated.strip() if speaker_separated.strip() else full_transcript.strip()
        
        return {
            "status": "success",
            "full_text": selected_text,
            "timestamp": "2025-07-28T16:44:07",
            # ë””ë²„ê¹…ìš© ì •ë³´
            "options": {
                "full_transcript_length": len(full_transcript.strip()),
                "speaker_separated_length": len(speaker_separated.strip()), 
                "raw_content_length": len(raw_content)
            }
        }
        
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None


def print_section(title: str, content: str):
    """ì„¹ì…˜ ì¶œë ¥ í•¨ìˆ˜"""
    print("\n" + "="*80)
    print(f" {title}")
    print("="*80)
    print(content)


def save_comparison_results(results: dict):
    """ë¹„êµ ê²°ê³¼ë¥¼ ê°œë³„ íŒŒì¼ë¡œ ì €ì¥"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # data ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs("data", exist_ok=True)
    
    model_comparison = results.get("model_comparison", {})
    
    # OpenAI ê²°ê³¼ íŒŒì¼ ì €ì¥
    if "openai_analysis" in model_comparison:
        openai_filename = f"openai_gpt_result_{timestamp}.md"
        openai_filepath = os.path.join("data", openai_filename)
        
        with open(openai_filepath, "w", encoding="utf-8") as f:
            f.write("# OpenAI GPT íšŒì˜ ë¶„ì„ ê²°ê³¼\n\n")
            f.write(f"ìƒì„± ì‹œê°„: {timestamp}\n\n")
            f.write("---\n\n")
            f.write(model_comparison["openai_analysis"])
        
        print(f"ğŸ’¾ OpenAI GPT ê²°ê³¼ ì €ì¥: {openai_filepath}")
    
    # Vertex AI ê²°ê³¼ íŒŒì¼ ì €ì¥
    if "vertexai_analysis" in model_comparison:
        vertexai_filename = f"vertexai_gemini_result_{timestamp}.md"
        vertexai_filepath = os.path.join("data", vertexai_filename)
        
        with open(vertexai_filepath, "w", encoding="utf-8") as f:
            f.write("# Google Vertex AI Gemini íšŒì˜ ë¶„ì„ ê²°ê³¼\n\n")
            f.write(f"ìƒì„± ì‹œê°„: {timestamp}\n\n")
            f.write("---\n\n")
            f.write(model_comparison["vertexai_analysis"])
        
        print(f"ğŸ’¾ Vertex AI Gemini ê²°ê³¼ ì €ì¥: {vertexai_filepath}")


def test_qa_extraction(questions: List[str], transcript_file: str):
    """Q&A ì¶”ì¶œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("\nğŸ” Q&A ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print(f"ğŸ“‹ ì§ˆë¬¸ ìˆ˜: {len(questions)}")
    
    # ì „ì‚¬ íŒŒì¼ ë¡œë“œ
    stt_data = load_sample_transcript(transcript_file)
    if not stt_data:
        print("âŒ ì „ì‚¬ íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    transcript = stt_data['full_text']
    print(f"ğŸ“„ ì „ì‚¬ ë‚´ìš© ê¸¸ì´: {len(transcript)}ì")
    
    # Vertex AI ì´ˆê¸°í™”
    try:
        llm = ChatVertexAI(
            project=GOOGLE_CLOUD_PROJECT,
            location=GOOGLE_CLOUD_LOCATION,
            model_name=VERTEX_AI_MODEL,
            temperature=VERTEX_AI_TEMPERATURE,
            max_output_tokens=VERTEX_AI_MAX_TOKENS
        )
        print("âœ… Vertex AI Gemini ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        return
    
    # ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    questions_text = "\n".join([f"{i+1}. {q}" for i, q in enumerate(questions)])
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = QA_EXTRACTION_PROMPT.format(
        questions=questions_text,
        transcript=transcript
    )
    
    # Q&A ì¶”ì¶œ ì‹¤í–‰
    print("\nğŸ”„ Q&A ì¶”ì¶œ ì¤‘...")
    try:
        message = HumanMessage(content=prompt)
        response = llm.invoke([message])
        qa_result = response.content
        
        # ê²°ê³¼ ì¶œë ¥
        print_section("Q&A ì¶”ì¶œ ê²°ê³¼", qa_result)
        
        # ê²°ê³¼ ì €ì¥
        save_qa_result(qa_result, questions, len(transcript))
        
        return qa_result
        
    except Exception as e:
        print(f"âŒ Q&A ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return None


def save_qa_result(qa_result: str, questions: List[str], transcript_length: int):
    """Q&A ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # data ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs("data", exist_ok=True)
    
    # Q&A ê²°ê³¼ íŒŒì¼ ì €ì¥
    qa_filename = f"qa_result_{timestamp}.md"
    qa_filepath = os.path.join("data", qa_filename)
    
    with open(qa_filepath, "w", encoding="utf-8") as f:
        f.write("# Q&A ì§ˆë¬¸ë³„ ë‹µë³€ ì •ë¦¬ ê²°ê³¼\n\n")
        f.write(f"ìƒì„± ì‹œê°„: {timestamp}\n")
        f.write(f"ì´ ì§ˆë¬¸ ìˆ˜: {len(questions)}\n")
        f.write(f"ì „ì‚¬ ë‚´ìš© ê¸¸ì´: {transcript_length}ì\n\n")
        f.write("---\n\n")
        f.write("## ì§ˆë¬¸ ëª©ë¡\n")
        for i, q in enumerate(questions, 1):
            f.write(f"{i}. {q}\n")
        f.write("\n---\n\n")
        f.write("## Q&A ê²°ê³¼\n\n")
        f.write(qa_result)
        f.write(f"\n\n---\n\n")
        f.write("## ë¶„ì„ ì •ë³´\n")
        f.write(f"- ëª¨ë¸: {VERTEX_AI_MODEL}\n")
        f.write(f"- ìƒì„± ì‹œê°„: {timestamp}\n")
    
    print(f"ğŸ’¾ Q&A ê²°ê³¼ ì €ì¥: {qa_filepath}")


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸš€ LLM í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("ì„ íƒí•˜ì„¸ìš”:")
    print("1. LLM ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ (OpenAI vs Gemini)")
    print("2. Q&A ì§ˆë¬¸ë³„ ë‹µë³€ ì¶”ì¶œ í…ŒìŠ¤íŠ¸")
    
    choice = input("\nì„ íƒ (1 ë˜ëŠ” 2): ").strip()
    
    # ì‹¤ì œ ì „ì‚¬ íŒŒì¼ ê²½ë¡œ
    transcript_file = "/Users/kimjoonhee/Documents/Orblit_1on1_AI/test_1on1.txt"
    
    if choice == "1":
        # ê¸°ì¡´ LLM ë¹„êµ í…ŒìŠ¤íŠ¸
        print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: OpenAI GPT vs Google Vertex AI Gemini")
        print(f"ğŸ“„ ì „ì‚¬ íŒŒì¼ ë¡œë“œ ì¤‘: {transcript_file}")
        
        stt_data = load_sample_transcript(transcript_file)
        if not stt_data:
            print("âŒ ì „ì‚¬ íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"âœ… ì „ì‚¬ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        print(f"   - ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(stt_data['full_text'])}ì")
        
        # MultiLLMAnalyzer ì´ˆê¸°í™”
        print("\nğŸ”§ LLM ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        analyzer = MultiLLMAnalyzer()
        
        # STT ê²°ê³¼ë¥¼ ë‘ ëª¨ë¸ë¡œ ë¶„ì„
        print("\nğŸ”„ STT ê²°ê³¼ ë¶„ì„ ì¤‘...")
        comparison_results = analyzer.analyze_stt_with_comparison(stt_data)
        
        # ê²°ê³¼ ì¶œë ¥
        model_comparison = comparison_results.get("model_comparison", {})
        
        if "error" in model_comparison:
            print(f"âŒ ì˜¤ë¥˜: {model_comparison['error']}")
            return

        # ê²°ê³¼ ì €ì¥
        save_comparison_results(comparison_results)
        print("\nâœ… LLM ë¹„êµ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    elif choice == "2":
        # Q&A í…ŒìŠ¤íŠ¸
        print("\nğŸ“‹ Q&A ì§ˆë¬¸ë³„ ë‹µë³€ ì¶”ì¶œ í…ŒìŠ¤íŠ¸")
        
        # 1on1 ë¯¸íŒ… ì§ˆë¬¸ë“¤
        sample_questions = [
            "2ë¶„ê¸° ì„±ê³¼ ê²€í† ì™€ 3ë¶„ê¸° ê³„íšì€?",
            "ì–´ë ¤ì› ë˜ ì ì´ë‚˜ ì•„ì‰¬ì› ë˜ ë¶€ë¶„ì€?",
            "ê·¹ë³µí•œ ë°©ë²•",
            "êµ¬ì²´ì ì¸ ì¼ì •ì´ë‚˜ ë§ˆì¼ìŠ¤í†¤",
            "ì˜¬í•´ ì–´ë–¤ ëª©í‘œë¥¼ ì„¸ì› ëŠ”ê°€?",
            "ìµœê·¼ ê°œì¸ì ì¸ ëª©í‘œì™€ ê´€ì‹¬ì‚¬ëŠ”?",
            "ê¶ê¸ˆí•œ ì ì´ë‚˜ ê±´ì˜ì‚¬í•­"
        ]
        
        print("ğŸ“ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ëª©ë¡:")
        for i, q in enumerate(sample_questions, 1):
            print(f"   {i}. {q}")
        
        print("\nğŸ’¡ ì§ˆë¬¸ì„ ìˆ˜ì •í•˜ë ¤ë©´ ì½”ë“œì—ì„œ sample_questions ë¦¬ìŠ¤íŠ¸ë¥¼ í¸ì§‘í•˜ì„¸ìš”.")
        
        # Q&A ì¶”ì¶œ ì‹¤í–‰
        qa_result = test_qa_extraction(sample_questions, transcript_file)
        
        if qa_result:
            print("\nâœ… Q&A ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        else:
            print("\nâŒ Q&A ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")
    
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1 ë˜ëŠ” 2ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
    


if __name__ == "__main__":
    main()